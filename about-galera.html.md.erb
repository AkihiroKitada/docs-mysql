---
title: About Highly Available Clusters
owner: MySQL
---

<strong><%= modified_date %></strong>

<%= partial "./galera_beta" %>

This topic describes how highly available (HA) clusters work in MySQL for Pivotal Cloud Foundry (PCF)
and contains information to help users decide whether to  use HA clusters.

## <a id="understanding"></a> Understanding Highly Available Clusters

By default, HA clusters splits large `Load Data` commands into small manageable units. This deviates from the standard behavior for MySQL. For more information, see [wsrep\_load\_data\_splitting] (https://www.percona.com/doc/percona-xtradb-cluster/LATEST/wsrep-system-index.html#wsrep_load_data_splitting) in the Percona XtraDB Cluster documentation.

<!-- LINK OUT TO AVAILABILITY OPTIONS PAGE -->

## <a id="requirements"></a> Infrastructure Requirements for Highly Available Deployments

### <a id="cap-plan"></a> Capacity Planning

When calculating IaaS usage, you must take into account that each HA instance requires three VMs. Therefore, the resources used for an HA plan must be tripled. For more information, see [Resource Usage Planning for On-Demand Plans](./recommended.html#resources).


### <a id="avail-zones"></a> Availability Zones

To minimize impact of an AZ outage and to remove single points of failure, Pivotal recommends that you provision three AZs if using HA deployments. With three AZs, the MySQL VMs are deployed to two AZs, and the broker is deployed to a third.


### <a id="network-req"></a> Networking Rules
In addition to the standard networking rules needed for MySQL for PCF, the operator must ensure HA
cluster specific network rules are also configured.

For information about HA cluster specific networking rules, see [Required Networking Rules for Highly Available
Cluster Plans](./about.html#hac-ports).

## <a id="limitations"></a> Highly Available Cluster Limitations

When deployed with HA topology, MySQL for PCF runs three nodes. This cluster arrangement imposes some limitations which do not apply to single node or leader-follower MySQL database servers. For more information about the difference between single node, leader-follower, and HA cluster topologies, see [Availability Options](./availability-options.html).

HA clusters perform validations at startup and during runtime to prevent you from using MySQL features that are not supported. If a validation fails during startup, the server is halted and throws an error. If a validation fails during runtime the operation is denied and throws an error.

These validations are designed to ensure optimal operation for common cluster setups that do not require experimental features and do not rely on operations not supported by HA clusters.

For more information about HA limitations, see [Percona XtraDB Cluster Limitations](https://www.percona.com/doc/percona-xtradb-cluster/LATEST/limitation.html) in the Percona XtraDB Cluster documentation.

### <a id="dev-limit"></a> Limitations for Developers

#### <a id="dev-storage"></a> Storage Engine Limitations 

- HA clusters only supports the InnoDB storage engine. The InnoDB is the default storage engine for new tables. Pre-existing tables that are not using InnoDB are at risk because they are not replicated within a cluster.

- Large DDL statements can lock all schemas. This can be mitigated by using the Rolling Schema Upgrade (RSU) method. For more information about using RSU to apply DDL statements, see [Using Rolling Schema Upgrade](./rsu.html).

#### <a id="dev-arch"></a> Architecture Limitations

- All tables must have a primary key. You can use multi-column primary keys. This is because HA clusters replicate using row based replication and ensure unique rows on each instance.

- Explicit table locking is not supported. For more information, see [EXPLICIT TABLE LOCKING](https://www.percona.com/doc/percona-xtradb-cluster/LATEST/features/pxc-strict-mode.html#explicit-table-locking) in the Percona XtraDB Cluster documentation.

- By default, auto-increment variables are not sequential and each node has gaps in IDs. This prevents auto-increment replication conflicts across the cluster. For more information, see [wsrep\_auto\_increment\_control](https://www.percona.com/doc/percona-xtradb-cluster/LATEST/wsrep-system-index.html#wsrep_auto_increment_control) in the Percona XtraDB Cluster documentation.

- HA clusters sets users to `READ ONLY` when using auto increment variables. This is because, otherwise HA clusters would require shared locking of the auto increment variables across the cluster, which causes it to be slower and less reliable.

- In InnoDB, some transactions can cause deadlocks. You can minimize deadlocks in your apps by rewriting transactions and SQL statements. For more information about deadlocks, see [Deadlocks in InnoDB](http://dev.mysql.com/doc/refman/en/innodb-deadlocks.html) and [How to Minimize and Handle Deadlocks](http://dev.mysql.com/doc/refman/en/innodb-deadlocks-handling.html) in MySQL documentation.

- Table partitioning can cause an hung state in the cluster. This is as a result of the implicit table locks that are used when running table partition commands. [TALK TO ANDREW]

### <a id="op-limit"></a> Limitations for Operators

#### <a id="network-limit"></a> Networking Limitations

- Database servers are shared and managed by multi-tenant processes to serve apps across the deployment. Although data is securely isolated between tenants using unique credentials, app performance can be impacted by noisy neighbors.

- Round-trip latency between database nodes must be less than five seconds. Latency exceeding this results in a network partition. If more than half of the nodes are partitioned, the cluster loses quorum and becomes unusable until manually bootstrapped. For more information about bootstrapping HA clusters, see [Bootstrapping](./bootstrapping.html).

- Although two proxy instances are deployed by default, there is no automation to direct clients from one to the other. 

#### <a id="op-storage"></a> Storage Engine Limitations 


- Large transaction sizes can inhibit the performance of the cluster and apps using the cluster. In a HA cluster, writes are processed as “a single memory-resident buffer”.

- You cannot execute a DML statement in parallel with a DDL statement, if both statements affect the same tables. If they are expected in parallel, the DML and DDL statements are both applied immediately to the cluster, causing errors.
  
## <a id="proxy"></a>Understanding the MySQL Proxy

<!-- acknowledge this exists, give picture  -->

MySQL for Pivotal Cloud Foundry (PCF) uses a proxy to send client connections
to the healthy MySQL database cluster nodes in a highly available cluster plan.
Using a proxy gracefully handles failure of nodes, enabling fast,
failover to other nodes within the cluster. When a 
node becomes unhealthy, the proxy closes all connections to the unhealthy node
and re-routes all subsequent connections to a healthy node.

The proxy used in MySQL for PCF is Switchboard.
Switchboard was developed to replace HAProxy as the proxy tier for the high availability cluster for MySQL databases in PCF.

<%= image_tag "images/switchboard-all-healthy.png" %>

Switchboard offers the following features: 

- **MySQL Server Access**

    MySQL clients communicate with nodes through this network port.
    These connections are automatically passed through to the nodes.

- **Switchboard and API**

    Operators can connect to Switchboard to view the state of the nodes.
    For more information about monitoring proxy health status, see 
    [Monitoring Node Health](./monitor-health.html).
    
