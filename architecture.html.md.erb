---
title: Architecture
owner: MySQL
---

This topic describes the high-level component architecture of the MySQL for PCF service, in both its single-node and HA configurations. It also describes the defaults and other design decisions applied to the service's MariaDB servers and the Galera Cluster that manages them.

## <a id="topology"></a> HA Topology ##

In HA topology, the MySQL for PCF service uses the following:

* Three MySQL servers running [MariaDB](https://mariadb.com/kb/en/mariadb/what-is-mariadb-galera-cluster/) as the MySQL engine and [Galera](http://galeracluster.com) to keep them in sync

* Two [Switchboard](https://github.com/cloudfoundry-incubator/switchboard) proxies that direct queries to a single backend, and provide superfast failover in the event of a backend server failure

* A customer-provided load balancer that directs traffic to one of the proxies, or to the other proxy in the event of proxy failure

* Two service brokers that create and bind service instances with failover in the event of a service broker failure

If you use multiple availability zones, the MySQL installer spreads MySQL and Proxy components across zones to make the service durable in the event of a zone failure.

![Quorum](topology.png)

### <a id="how-it-works"></a> How It Works

Normally, MySQL for PCF only sends queries to one of the three MySQL servers at a time, the _current master_ node. Galera ensures that the other two servers mirror any changes made on the current master.

In a three-node Galera cluster, two nodes define a quorum that automatically maintains availability when any node becomes inaccessible. This happens when the host fails or in the event of a network partition.

When a single node cannot communicate with the rest of the cluster, it stops accepting traffic. The other two nodes continue to function normally. When the isolated node regains connectivity, it rejoins the cluster.

If two nodes are no longer able to connect to the cluster, quorum is lost and  the cluster is no longer accessible. To bring the cluster back up you must restart it manually, as documented in the [bootstrapping docs](bootstrapping.html).

### <a id="even-number"></a> Avoid an even number of nodes ###

  - It is generally recommended to avoid an even number of nodes. This is because a partition could cause the entire cluster to lose quorum, as neither remaining component has more than half of the total nodes.
  - A 2 node cluster cannot tolerate the failure of single node failure as this would cause loss of quorum. As such, the minimum number of nodes required to tolerate single node failure is 3.

## <a id="defaults"></a> MySQL Server Defaults 

This section describes the defaults that the MySQL for PCF tile applies to its Galera and MariaDB components.

### SST method

Galera supports multiple methods for [State Snapshot Transfer](http://www.percona.com/doc/percona-xtradb-cluster/5.5/manual/state_snapshot_transfer.html).
The `rsync` method is usually fastest. The `xtrabackup` method has the advantage of keeping the donor node writeable during SST. We have chosen to use `xtrabackup`.

### InnoDB Log Files
Our cluster defaults to 1GB for log file size to support larger blob.

### Max User Connections
To ensure all users get fair access to system resources, we default each user's number of connections to 40. Operators can override this setting, per plan, in the Service Plans configuration pane.

### Skip External Locking
Since each Virtual Machine only has one mysqld process running, we do not need external locking.

### Max Allowed Packet
We allow blobs up to 256MB. This size is unlikely to limit a user's query, but is also manageable for our InnoDB log file size. Consider using our Riak CS service for storing large files.

### Innodb File Per Table
Innodb allows using either a single file to represent all data, or a separate file for each table. We chose to use a separate file for each table as this provides more flexibility and optimization. For a full list of pros and cons, see MySQL's documentation for [InnoDB File-Per-Table Mode](http://dev.mysql.com/doc/refman/5.5/en/innodb-multiple-tablespaces.html).

### Innodb File Format
To take advantage of all the extra features available with the `innodb_file_per_table = ON` option, we use the `Barracuda` file format.

### Temporary Tables
MySQL is configured to convert temporary in-memory tables to temporary on-disk tables when a query EITHER generates more than 16 million rows of output or uses more than 32MB of data space.
Users can see if a query is using a temporary table by using the EXPLAIN command and looking for "Using temporary," in the output.
If the server processes very large queries that use /tmp space simultaneously, it is possible for queries to receive no space left errors.

### Reverse Name resolution
Since our connection authentication is implemented using user credentials, the default implementation does not restrict to host names from which a user may connect. To save time on each new connection, we've turned off reverse name resolution by default.

### Large Data File Ingestion
Now that a major bug has been fixed in MariaDB, we enable `wsrep_load_data_splitting` which splits large data imports into separate transactions to enable loading data into a MariaDB cluster.

## <a id="behavior"></a> Cluster Behavior ##

This section explains how the MySQL cluster behaves when you change the number of nodes.

### Scaling up from 1 to 3 nodes

When a new MariaDb node comes online, it replicates data from the existing node in the cluster. Once replication is complete, the node will join the cluster. The proxy will continue to route all incoming connections to the primary node while it remains healthy.

If the proxy detects that this node becomes [unhealthy](https://github.com/cloudfoundry/cf-mysql-release/blob/release-candidate/docs/proxy.md#unhealthy), it will sever existing connections, and route all new connections to a different, healthy node. If there are no healthy MariaDb nodes, the proxy will reject all subsequent connections.

While transitioning from one node to a cluster, there will be an undetermined period of performance degradation while the new node syncs all data from the original node.

Note: If you are planning to scale up MariaDb nodes, it is recommended to do so in different Availability Zones to maximize cluster availability. An Availability Zone is a network-distinct section of a given Region. Further details are available in [Amazon's documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html).

### Scaling down from 3 to 1 node

When scaling from multiple nodes to a single MariaDb node, the proxy will determine that the sole remaining node is the primary node (provided it remains healthy). The proxy routes incoming connections to the remaining MariaDb node.

### <a id="removal"></a> Graceful removal of a node ###
  - Manually shutting down a node with `monit` (or decreasing cluster size by one) will cause the node to gracefully leave the cluster.
  - Cluster size is reduced by one and maintains healthy state. Cluster will continue to operate, even with a single node, as long as other nodes left gracefully.

### <a id="rejoin"></a> Rejoining the cluster (existing nodes)

Existing nodes restarted with monit should automatically join the cluster. If an existing node fails to join the cluster, it may be because its transaction record's (`seqno`) is higher than that of the nodes in the cluster with quorum (aka the primary component).

  - If the node has a higher `seqno` it will be apparent in the error log `/var/vcap/sys/log/mysql/mysql.err.log`.
  - If the healthy nodes of a cluster have a lower transaction record number than the failing node, it might be desirable to shut down the healthy nodes and bootstrap from the node with the more recent transaction record number. See the [bootstrapping docs](bootstrapping.html) for more details.
  - Manual recovery may be possible, but is error-prone and involves dumping transactions and applying them to the running cluster (out of scope for this doc).
  - Abandoning the data is also an option, if you're ok with losing the unsynchronized transactions. Follow the following steps to abandon the data (as root):
    - Stop the process with `monit stop mariadb_ctrl`.
    - Delete the galera state (`/var/vcap/store/mysql/grastate.dat`) and cache (`/var/vcap/store/mysql/galera.cache`) files from the persistent disk.
    - Restarting the node with `monit start mariadb_ctrl`.

### <a id='state-snapshot-transfer-sst'></a>State Snapshot Transfer (SST)

When a new node is added to the cluster or rejoins the cluster, it synchronizes state with the primary component via a process called SST. A single node from the primary component is chosen to act as a state donor. By default Galera uses rsync to perform SST, which blocks for the duration of the transfer. However, MySQL for Pivotal Cloud Foundry (PCF) is configured to use [Xtrabackup](http://www.percona.com/doc/percona-xtrabackup), which allows the donor node to continue to accept reads and writes.

