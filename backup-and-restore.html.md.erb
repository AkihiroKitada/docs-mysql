---
title: Backing Up and Restoring On-Demand MySQL for VMware Tanzu
owner: MySQL
---

<strong><%= modified_date %></strong>

This topic describes how to configure automated backups for <%= vars.product_full %>
and how to manually restore a MySQL service instance from a backup.

<p class="note"><strong>Note:</strong>
  You must configure automated backups. Automated backups cannot be disabled.
</p>
<p class="note"><strong>Note:</strong> <%= vars.recommended_by %>
  that you always configure a single instance plan in order to streamline the
  restore process for leader-follower plans.
</p>

Additionally, developers can create logical backups using `mysqldump`.
For information, see [Backing Up and Restoring with mysqldump](./backup-mysqldump.html).

## <a id="overview"></a>About Automated Backups

Automated backups do the following:

- Periodically create and upload backups suitable for restoring all of the databases used
by the service instance.
- Operate without locking apps out of the database. There is no downtime.
- Include a metadata file that contains the critical details of the backup,
including the calendar time of the backup.
- Encrypt backups within the <%= vars.product_short %> VM.
Unencrypted data is never transported outside the <%= vars.product_short %> deployment.

### <a id="understanding-metadata"></a> Backup Files and Metadata

When <%= vars.product_short %> runs a backup,
it uploads two files with Unix epoch-timestamped filenames of the form `mysql-backup-TIMESTAMP`:

  * The encrypted data backup file `mysql-backup-TIMESTAMP.tar.gpg`
  * A metadata file `mysql-backup-TIMESTAMP.txt`

The metadata file contains information about the backup and looks like this:

```
ibbackup_version = 2.4.5
end_time = 2017-04-24 21:00:03
lock_time = 0
binlog_pos =
incremental = N
encrypted = N
server_version = 5.7.16-10-log
start_time = 2017-04-24 21:00:00
tool_command = --user=admin --password=... --stream=tar tmp/
innodb_from_lsn = 0
innodb_to_lsn = 2491844
format = tar
compact = N
name =
tool_name = innobackupex
partial = N
compressed = N
uuid = fd13cf26-2930-11e7-871e-42010a000807
tool_version = 2.4.5
```

The most important entires in the metadata file are the following:

+ `start_time`: You use this to determine which transactions are captured in the backup.
+ `server_version`: You use this to determine potential incompatibilities
when restoring an instance with this backup artifact.

The backup process does not interrupt MySQL service,
however backups only reflect transactions that are completed before the `start_time`.

<p class="note"><strong>Note</strong>: Although both <code>compressed</code> and <code>encrypted</code>
  show as <code>N</code> in this file, the backup uploaded by <%= vars.product_short %>
  is both compressed and encrypted.
</p>

### <a id="enable-backups"></a> About Configuring Automated Backups

<%= vars.product_short %> automatically backs up databases to external storage.

* **How and where**---There are four options for how automated backups transfer backup data
and where the data saves to:
  - [Option 1: Back up with SCP](#scp)---<%= vars.product_short %> runs an SCP command
  that secure-copies backups to a VM
    or physical machine operating outside of your deployment.
    SCP stands for secure copy protocol, and offers a way to securely transfer files between two hosts.
    The operator provisions the backup machine separately from their installation.
    This is the fastest option.
  - [Option 2: Back up to Ceph or S3](#ceph-or-s3)---<%= vars.product_short %> runs an
  [Amazon S3](https://aws.amazon.com/documentation/s3/) client that saves backups to an S3 bucket,
    a [Ceph](http://docs.ceph.com/docs/master/) storage cluster,
    or another S3-compatible endpoint certified by VMware.
  - [Option 3: Back up to GCS](#gcs)---<%= vars.product_short %> runs a
    [GCS](https://cloud.google.com/storage/) SDK that saves backups to a Google Cloud Storage bucket.
  - [Option 4: Back up to Azure Storage](#azure)---<%= vars.product_short %> runs an
    [Azure](https://docs.microsoft.com/en-us/azure/storage/) SDK that saves backups to
    an Azure storage account.

* **When**---Backups follow a schedule that you specify with a
[cron](http://godoc.org/github.com/robfig/cron) expression.

* **What is backed up**---Each MySQL instance backs up its entire MySQL data directory,
consistent to a specific point in time.

To configure automated backups,
follow the procedures below according to the option you choose for external storage.

## <a id="scp"></a> Option 1: Back Up with SCP

SCP enables the operator to use any desired storage solution on the destination VM.

To back up your database using SCP, complete the following procedures:

+ [Create a Public and Private Key Pair](#scp-keys)
+ [Configure Backups in <%= vars.ops_manager %>](#configure-scp)

### <a id="scp-keys"></a> Create a Public and Private Key Pair

<%= vars.product_short %> accesses a remote host as a user with a private key for authentication.
<%= vars.recommended_by %> that this user and key pair be solely for <%= vars.product_short %>.

1. Determine the remote host that you will be using to store backups for <%= vars.product_short %>.
   Ensure that the MySQL service instances can access the remote host.

    <p class="note"><strong>Note</strong>: <%= vars.recommended_by %> using a VM outside your deployment
      for the destination of SCP backups. As a result,
      you might need to enable public IPs for the MySQL VMs.
    </p>

1. (Recommended) Create a new user for <%= vars.product_short %> on the destination VM.

1. (Recommended) Create a new public and private key pair for authenticating as the above user
on the destination VM.

### <a id="configure-scp"></a> Configure Backups in <%= vars.ops_manager %>

Use <%= vars.ops_manager %> to configure <%= vars.product_short %> to backup using SCP.

1. In <%= vars.ops_manager %>, open the <%= vars.product_short %> tile **Backups** pane.

1. Select **SCP**.<br/>
    ![SCP Backup Configuration Form](scp-backups.png)

1. Configure the fields as follows:

    <table>
        <tr>
            <th>Field</th>
            <th>Instructions</th>
        </tr>
        <tr>
            <td>
                <strong>Username</strong>
            </td>
            <td>
                Enter the user that you created in
                <a href="#scp-keys">Create a Public and Private Key Pair</a>
                above.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Private Key</strong>
            </td>
            <td>
                 Enter the private key that you created above.
                 The public key should be stored for ssh and scp access on the destination VM.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Hostname</strong>
            </td>
            <td>
                Enter the IP or DNS entry that should be used to access the destination VM.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Destination Directory</strong>
            </td>
            <td>
                Enter the directory in which <%= vars.product_short %> should upload backups.
            </td>
        </tr>
        <tr>
            <td>
                <strong>SCP Port</strong>
            </td>
            <td>
               Enter the SCP port number for SSH.
               Usually, this is port 22.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Cron Schedule</strong>
            </td>
            <td>
                Enter a <code>cron</code> schedule, using standard
                <a href="https://help.ubuntu.com/community/CronHowto">
                cron syntax</a>, to take backups of each service instance.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Fingerprint</strong>
            </td>
            <td>
                (Optional) Enter the fingerprint of the destination VM's public key.
                This helps to detect any changes to the destination VM.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Enable Email Alerts</strong>
            </td>
            <td>
                Select to receive email notifications when a backup failure occurs.
                Also verify that:
                <ul>
                 <li>All users who need to be notified about failed backups
                     have the Space Developer role in the system org and system space.</li>
                 <li>Email notifications are configured in
                   <%= vars.app_runtime_full %> (<%= vars.app_runtime_abbr %>).
                     See <a href="https://docs.pivotal.io/platform/customizing/configure-pas.html#smtp">
                     (Optional) Configure Email Notifications</a>.
                 </li>
                </ul>
            </td>
        </tr>
      </table>

## <a id="ceph-or-s3"></a> Option 2: Back Up to Ceph or Amazon S3

To back up your database on Ceph or Amazon S3, complete the following procedures:

+ [Create a Custom Policy and Access Key](#access-key-aws)
+ [Configure Backups in <%= vars.ops_manager %>](#configure-aws)

### <a id="access-key-aws"></a> Create a Custom Policy and Access Key

<%= vars.product_short %> accesses your S3 bucket through a user account.
<%= vars.recommended_by %> that this account is only used by <%= vars.product_short %>.
You must apply a minimal policy that enables the user account upload backups to your S3 bucket.

The procedure in this section assumes that you are using a AWS S3 bucket. If you are using a Ceph or another S3-compatible bucket to create the policy and access key, follow the documentation for your storage solution.

Give the policy the following permissions:

* List and upload to buckets
* (Optional) Create buckets in order to use buckets that do not already exist

To create a policy and access key in AWS:

1. Create a policy for your <%= vars.product_short %> user account.

    In AWS, create a new custom policy by following the procedure in the
    [AWS documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create-console.html#access_policies_create-json-editor). <br>
    Paste in the following permissions:

    ```json
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Sid": "ServiceBackupPolicy",
          "Effect": "Allow",
          "Action": [
            "s3:ListBucket",
            "s3:ListBucketMultipartUploads",
            "s3:ListMultipartUploadParts",
            "s3:CreateBucket",
            "s3:PutObject"
          ],
          "Resource": [
            "arn:aws:s3:::MY_BUCKET_NAME/*",
            "arn:aws:s3:::MY_BUCKET_NAME"
          ]
        }
      ]
    }
```

1. Record the Access Key ID and Secret Access Key user credentials for a new user account by following the procedure in
the [AWS documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console).
Ensure you select **Programmatic access**
and **Attach existing policies to user directly**. You must attach the policy you created in the previous step.

### <a id="configure-aws"></a> Configure Backups in <%= vars.ops_manager %>

Use <%= vars.ops_manager %> to connect <%= vars.product_short %> to your S3 account.

1. In <%= vars.ops_manager %>, open the <%= vars.product_short %> tile **Backups** pane.

1. Select **Ceph or Amazon S3**.

    ![S3 Backup Configuration Form](S3-backups.png)

1. Configure the fields as follows:

    <table>
        <tr>
            <th>Field</th>
            <th>Instructions</th>
        </tr>
        <tr>
            <td>
                <strong>Access Key ID</strong> and <strong>Secret Access Key</strong>
            </td>
            <td>
                Enter the S3 Access Key ID and Secret Access Key from <a href="#access-key-aws">Create
                a Custom Policy and Access Key</a> above.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Endpoint URL</strong>
            </td>
            <td>
                Enter the S3 compatible endpoint URL for uploading backups.
                URL must start with <code>http://</code> or <code>https://</code>.
                The default is <code>https://s3.amazonaws.com</code>
            </td>
        </tr>
        <tr>
            <td>
                <strong>Region</strong>
            </td>
            <td>
                Enter the region where your bucket is located or the region where
                you want a bucket to be created.
                If the bucket does not already exist, it is created automatically.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Bucket Name</strong>
            </td>
            <td>
                Enter the name of your bucket.
                Do not include an <code>s3://</code> prefix, a trailing <code>/</code>, or underscores.
                <%= vars.recommended_by %> using the naming convention <code>DEPLOYMENT-backups</code>,
                such as <code>sandbox-backups</code>.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Bucket Path</strong>
            </td>
            <td>
               Enter the path in the bucket to store backups.
               Do not include a trailing <code>/</code>.
               <%= vars.recommended_by %> using <code>mysql-v2</code>.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Cron Schedule</strong>
            </td>
            <td>
                Enter a <code>cron</code> schedule, using standard
                <a href="https://help.ubuntu.com/community/CronHowto">cron syntax</a>,
                to take backups of each service instance.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Enable Email Alerts</strong>
            </td>
            <td>
                Select to receive email notifications when a backup failure occurs.
                Also verify that:
                <ul>
                 <li>All users who need to be notified about failed backups
                     have the Space Developer role in the system org and system space.</li>
                 <li>Email notifications are configured in <%= vars.app_runtime_abbr %>.
                     See <a href="https://docs.pivotal.io/platform/customizing/configure-pas.html#smtp">
                     (Optional) Configure Email Notifications</a>.
                 </li>
                </ul>
            </td>
        </tr>
      </table>

## <a id="gcs"></a> Option 3: Back Up to GCS

To back up your database on Google Cloud Storage (GCS), complete the following procedures:

+ [Create a Service Account and Private Key](#service-account-gcs)
+ [Configure Backups in <%= vars.ops_manager %>](#configure-gcs)

### <a id="service-account-gcs"></a> Create a Service Account and Private Key

<%= vars.product_short %> accesses your GCS store through a service account.
<%= vars.recommended_by %> that this account is only used by <%= vars.product_short %>.
You must apply a minimal policy that enales the service account upload backups to your GCS store.

The service account needs the following permissions:

* List and upload to buckets
* (Optional) Create buckets in order to use buckets that do not already exist

To create a service account and private key in GCS:

1. Create a new service account by following the procedure in the [GCS documentation](https://cloud.google.com/iam/docs/creating-managing-service-accounts#creating_a_service_account). <br>
When you create the service account, you must do the following:
      1. Enter a unique name for the service account name.
      1. Add the **Storage Admin** role.
      1. Create and download a private key JSON file.


### <a id="configure-gcs"></a> Configure Backups in <%= vars.ops_manager %>

Use <%= vars.ops_manager %> to connect <%= vars.product_short %> to your GCS account.

1. In <%= vars.ops_manager %>, open the <%= vars.product_short %> tile **Backups** pane.

1. Select **GCS**.

    ![GCS Backup Configuration Form](gcs-backups.png)

1. Configure the fields as follows:

    <table>
        <tr>
            <th>Field</th>
            <th>Instructions</th>
        </tr>
        <tr>
            <td>
                <strong>Project ID</strong>
            </td>
            <td>
                Enter the Project ID for the Google Cloud project that you are using.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Bucket name</strong>
            </td>
            <td>
                Enter the bucket name in which to upload.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Service Account JSON</strong>
            </td>
            <td>
                Enter the contents of the service account JSON file that you downloaded
              when creating a service account in <a href="#service-account-gcs">Create a Service Account and Private Key</a> above.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Cron Schedule</strong>
            </td>
            <td>
                Enter a <code>cron</code> schedule, using standard
                <a href="https://help.ubuntu.com/community/CronHowto">
                cron syntax</a>, to take backups of each service instance.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Enable Email Alerts</strong>
            </td>
            <td>
                Select to receive email notifications when a backup failure occurs.
                Also verify that:
                <ul>
                 <li>All users who need to be notified about failed backups
                     have the Space Developer role in the system org and system space.</li>
                 <li>Email notifications are configured in <%= vars.app_runtime_abbr %>.
                     See <a href="https://docs.pivotal.io/platform/customizing/configure-pas.html#smtp">
                     (Optional) Configure Email Notifications</a>.
                 </li>
                </ul>
            </td>
        </tr>
      </table>

## <a id="azure"></a> Option 4: Back Up to Azure Storage

To back up your database on Azure Storage, complete the following procedures:

+ [Create a Storage Account and Access Key](#storage-account-azure)
+ [Configure Backups in <%= vars.ops_manager %>](#configure-azure)

### <a id="storage-account-azure"></a> Create a Storage Account and Access Key

<%= vars.product_short %> accesses your Azure Storage account through a storage access key.
<%= vars.recommended_by %> that this account is only used by <%= vars.product_short %>.
You must apply a minimal policy that enables the storage account upload backups to your Azure Storage.

The storage account needs the following permissions:

* List and upload to buckets
* (Optional) Create buckets in order to use buckets that do not already exist

To create a storage account and access key:

1. Create a new storage account by following the procedure in the [Azure documentation](https://docs.microsoft.com/en-us/azure/storage/common/storage-account-create?tabs=azure-portal#create-a-storage-account).

1. View your access key by following the procedure in the [Azure documentation](https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage#view-access-keys-and-connection-string)

### <a id="configure-azure"></a> Configure Backups in <%= vars.ops_manager %>

Complete the following steps to back up your database to your Azure Storage account.

1. In <%= vars.ops_manager %>, open the <%= vars.product_short %> tile **Backups** pane.

1. Select **Azure**.

    ![Azure Backup Configuration Form](azure-backups.png)

1. Configure the fields as follows:

    <table>
        <tr>
            <th>Field</th>
            <th>Instructions</th>
        </tr>
        <tr>
            <td>
                <strong>Account</strong>
            </td>
            <td>
               Enter the Azure Storage account name that you created in <a href="#storage-account-azure">Create a Storage Account and Access Key</a> above.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Azure Storage Access Key</strong>
            </td>
            <td>
                Enter one of the storage access keys that you viewed in <a href="#storage-account-azure">Create a Storage Account and Access Key</a> above.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Container Name</strong>
            </td>
            <td>
                Enter the container name that backups should be uploaded to.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Destination Directory</strong>
            </td>
            <td>
                Enter the directory in which backups should be uploaded to inside of the Container.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Blob Store Base URL</strong>
            </td>
            <td>
               By default, backups are sent to the public Azure blob storage.
               To use an on-premise blob storage, enter the hostname of the blob storage.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Cron Schedule</strong>
            </td>
            <td>
                Enter a <code>cron</code> schedule, using standard
                <a href="https://help.ubuntu.com/community/CronHowto">cron syntax</a>,
                to take backups of each service instance.
            </td>
        </tr>
        <tr>
            <td>
                <strong>Enable Email Alerts</strong>
            </td>
            <td>
                Select to receive email notifications when a backup failure occurs.
                Also verify that:
                <ul>
                 <li>All users who need to be notified about failed backups
                     have the Space Developer role in the system org and system space.</li>
                 <li>Email notifications are configured in <%= vars.app_runtime_abbr %>.
                     See <a href="https://docs.pivotal.io/platform/customizing/configure-pas.html#smtp">
                     (Optional) Configure Email Notifications</a>.
                 </li>
                </ul>
            </td>
        </tr>
      </table>

## <a id="restore"></a> Restore a Service Instance from Backup

Restoring <%= vars.product_short %> from backup is a manual process primarily intended for
disaster recovery.
Restoring a <%= vars.product_short %> service instance replaces all of its data and running state.

To restore a <%= vars.product_short %> instance from an offsite backup,
download the backup and restore to a new instance by following these procedures:

+ [Retrieve Backup Encryption Key](#retrieve-key)
+ [Download the Backup Artifact](#download)
+ [Restore the Service Instance](#restoreinstance)

###<a id="retrieve-key"></a> Retrieve Backup Encryption Key

To retrieve the backup encryption key:

1. Find and record your CredHub credentials by following the procedure in [Find the CredHub Credentials](./prepare-tls.html#credhub-creds).

1. Find the GUID for the service instance that you want to restore by running:

    ```
    cf service MY-INSTANCE-NAME --guid
    ```
    Record the output.

1. Record the information needed to log in to the BOSH Director VM by following the procedure in
[Gather Credential and IP Address Information](https://docs.pivotal.io/pivotalcf/2-4/customizing/trouble-advanced.html#gather).

1. Log in to the <%= vars.ops_manager %> VM by following the procedure in
[Log in to the <%= vars.ops_manager %> VM with SSH](https://docs.pivotal.io/pivotalcf/2-4/customizing/trouble-advanced.html#ssh).

1. Set the API target of the CredHub CLI as your CredHub server by running:

    ```
    credhub api https://BOSH-DIRECTOR:8844 \
    --ca-cert=/var/tempest/workspaces/default/root_ca_certificate
    ```

    Where `BOSH-DIRECTOR` is the IP address of the BOSH Director VM.
    For example:
    <pre class="terminal">$ credhub api http<span>s:</span>//10.0.0.5:8844 \
      --ca-cert=/var/tempest/workspaces/default/root\_ca\_certificate</pre>

2. Log in to CredHub by running:

    ```
    credhub login \
    --client-name=CREDHUB-CLIENT-NAME \
    --client-secret=CREDHUB-CLIENT-SECRET
    ```
    Where
    * `CREDHUB-CLIENT-NAME` is the value you recorded for `BOSH_CLIENT` in
		[Find the CredHub Credentials in <%= vars.ops_manager %>](#credhub-creds) above.
    * `CREDHUB-CLIENT-SECRET` is the value you recorded for `BOSH_CLIENT_SECRET` in
		[Find the CredHub Credentials in <%= vars.ops_manager %>](#credhub-creds) above. <br>

    For example:
    <pre class="terminal">$ credhub login \
    	--client-name=credhub \
    	--client-secret=abcdefghijklm123456789</pre>
1. Use the CredHub CLI to retrieve the backup encryption key by running:

    ```
    credhub get \
    -n /p-bosh/service-instance_GUID/backup_encryption_key
    ```
    For example:
    <pre class="terminal">$ credhub get \
    -n /p-bosh/service-instance_70d30bb6-7f30-441a-a87c-05a5e4afff26/backup_encryption_key</pre>

1. Copy the backup encryption key under <code>value</code> in the output.
    You use this key when you restore the backup.

    For example:
    <pre class="terminal">id: d6e5bd10-3b60-4a1a-9e01-c76da688b847
    name: /p-bosh/service-instance_70d30bb6-7f30-441a-a87c-05a5e4afff26/backup_encryption_key
    type: password
    value: UMF2DXsqNPPlCNWMdVMcNv7RC3Wi10
    version_created_at: 2018-04-02T23:16:09Z</pre>


###<a id="download"></a> Download the Backup Artifact

These instructions assume that you are using AWS S3 as your backup destination.
If you are using a different backup destination,
see the documentation for your backup provider to download the backup.

<p class="note"><strong>Note</strong>:
  You can also use the AWS Management Console to download backups.
</p>

To download the backup artifact from an AWS S3 bucket using the command line:

1. From the <%= vars.ops_manager %> VM,
download the manifest for the service instance deployment by specifying the deployment name
as `service-instance_GUID` and a filename for the manifest.
For example:
  <pre class="terminal">
   $ bosh -e my-env \
   -d service-instance_12345678-90ab-cdef-1234-567890abcdef \
   manifest > ./manifest.yml
  </pre>

4. Record the following properties from the download manifest:
  * `properties.service-backup.destinations[0].config.bucket_name`:
  This is the bucket where the backups are uploaded.
  * `properties.service-backup.destinations[0].config.bucket_path`:
  This is the path within the bucket above.

5. Log in to the AWS CLI.
   For information about how to download and use the AWS CLI,
   see [AWS Command Line Interface](https://aws.amazon.com/cli/).

6. List the available backups for the instance by running:

    ```
    $ aws s3 ls \
    --recursive s3://BUCKET-NAME/BUCKET-PATH/service-instance_GUID/
    ```
    Where:
    + `BUCKET-NAME` is the bucket where the backups are uploaded recorded in the previous step.
    + `BUCKET-PATH` is the path within the bucket recorded in the previous step.

    The artifacts are sorted by time.

7. Select the most recent backup file or an older backup you want to restore from.
    The backups are timestamped in the filename and have a `.gpg` extension.

8. Download the selected backup by running:

    ```
    aws s3 cp \
    s3://BUCKET-NAME/BUCKET-PATH/service-instance_GUID/YEAR/MONTH/DATE/mysql-backup-TIMESTAMP.tar.gpg \
    ./a-local-path/mysql-backup-TIMESTAMP.tar.gpg
    ```

###<a id="restoreinstance"></a> Restore the Service Instance

<p class="note warning"><strong>Warning:</strong> Restoring a service instance is destructive.
<%= vars.recommended_by %> that you restore to a new and unused service instance.
</p>

To restore the downloaded backup to a new service instance for a single node, leader-follower,
or highly available (HA) cluster plan:

1. [Create and Prepare a New Service Instance for Restore](#prepare)
1. [Restore a Single Node or Leader-Follower Instance](#run-sn-leader-follower)
1. [Restore an HA Cluster Instance](#runHA)
1. [Restage the Service Instance](#restage)

#### <a id="prepare"></a> Create and Prepare a New Service Instance for Restore

To prepare a new service instance for restore:

1. To create a new MySQL service instance, run the following command:

    ```
    cf create-service p.mysql SERVICE-PLAN NEW-INSTANCE-NAME
    ```

    Where:
    + `SERVICE-PLAN` is the name of the service plan for your new service instance.
    + `NEW-INSTANCE-NAME` is the name of the new service instance.
    <br><br>

    <ul>
    <li>If you are using an HA cluster plan, you can only restore backup artifacts to an HA service instance.</li><br>

    <li>If you are using a single node or leader-follower plan,
      you can only restore backup artifacts to a single node service instance.
      The persistent disk size of this single node plan must be three times larger than
      the maximum size of the data developers need.
      For more information, see <a href="recommended.html#persistent"> Persistent Disk Usage</a>.</li><br>

    <li>If you want to restore to a leader-follower plan, you can do one of the following:
      <ul>
        <li><b>Create a single-node instance to restore to</b>
          then update the plan on this instance to leader-follower after you run the restore utility.
          For how to update the plan,
          see step 1 of <a href="#restage">Restage the Service Instance</a>.</li>
        <li><b>Create a leader-follower instance to restore to</b> then scale the instances to one
          before running the restore utility. For how to scale instances,
          see step 6 of this procedure.</li>
      </ul>
    </li>
</ul>



    For more information, see [Create a Service Instance](./use.html#create).

1. To monitor the status of the service instance creation, run the following command:

    ```
    cf service NEW-INSTANCE-NAME
    ```
    Where `NEW-INSTANCE-NAME` is the name of the new service instance. <br><br>

1. To locate and record the GUID associated with your new service instance, run the following command:

    ```
    cf service NEW-INSTANCE-NAME --guid
    ```
    Where `NEW-INSTANCE-NAME` is the name of the new service instance. <br><br>

1. To retrieve the admin password for your new service instance, do the procedure
   in [Retrieve Admin and Read-Only Admin Credentials for a Service Instance](troubleshoot.html#instance-creds).

1. From the <%= vars.ops_manager %> VM, to find and record the new instance name and GUID from BOSH,
run the following command:

    ```
    bosh -e ENVIRONMENT -d DEPLOYMENT instances
    ```

1. If you created a leader-follower or HA cluster service instance to restore to, do the following:
    1. Modify the deployment manifest to scale the instances to one. Do one of the following:
      * To scale a leader-follower instance, set the `instances` property to `1` and the
      `leader-follower` property to `false`.
      Update the instance group  and properties for your new service instance as follows:

            ```yaml
            instance_groups:
            - name: mysql
              ...
              instances: 1    # Scale instances to 1
              ...
            properties:
              leader_follower:
                enabled: false    # Set to false
              ...
            ```
      * To scale an HA cluster instance, modify the deployment manifest
      to set `instances` to `1`.
      Update the instance group for your new service instance as follows:

            ```yaml
            instance_groups:
            - name: mysql
              ...
              instances: 1    # Scale instances to 1
              ...
            ```
  1. Redeploy the deployment manifest by running:

        ```
        bosh -e ENVIRONMENT -d DEPLOYMENT deploy PATH-TO-MANIFEST.yml
        ```
        For example:

        <pre class="terminal">
        bosh -e my-env -d service-instance_12345678-90ab-cdef-1234-567890abcdef deploy ./manifest.yml
        </pre>
    <!-- HA probs the same for scaling. Andrew is going to confirm the manifest stuff -->
1. To copy the downloaded backup to the new service instance, run the following command:

    ```
    bosh -e ENVIRONMENT  \
    -d DEPLOYMENT scp mysql-backup-TIMESTAMP.tar.gpg BOSH-INSTANCE:DESTINATION-PATH
    ```
    Where:<br>
    - `BOSH-INSTANCE` is `mysql/INSTANCE-GUID`. For example, `mysql/d7ff8d46-c3e8-449f-aea7-5a05b0a1929c`.
    - `DESTINATION-PATH` is where the backup file saves on the BOSH VM. For example, `/tmp/`.

1. Use the BOSH CLI to SSH in to the newly created MySQL service instance.
For more information, see [BOSH SSH](https://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#bosh-ssh).

1. After securely logging in to MySQL, to become root, run the following command:

    ```
    sudo su
    ```

#### <a id="run-sn-leader-follower"></a> Restore a Single Node or Leader-Follower Instance

<p class="note warning"><strong>WARNING</strong>: This is a destructive action and
  should only be run on a new and unused service instance.</p>

Be sure you have followed the procedure in
[Create and Prepare a New Service Instance for Restore](#prepare) above.

To restore a single node or leader-follower service instance, do the following:

1. Run the restore utility to restore the backup artifact into the data directory. This process:
    * Deletes any existing data
    * Decrypts and untars the backup artifact
    * Restores the backup artifact into the MySQL data directory

    ```
    mysql-restore --encryption-key ENCRYPTION-KEY \
    --mysql-username admin --mysql-password ADMIN-PASSWORD --restore-file RESTORE-FILE-PATH
    ```
    Where:
    <ul>
      <li>`ENCRYPTION-KEY` is the backup encryption key you retrieved in
        [Retrieve Backup Encryption Key](#retrieve-key).</li>
      <li>`ADMIN-PASSWORD` is the admin password for your new service instance,
        retrieved in [Create and Prepare a New Service Instance for Restore](#prepare) above.</li>
      <li>`RESTORE-FILE-PATH` is the full path on the MySQL VM where the backup artifact exists.</li>
    </ul>
1. Exit the MySQL VM.


#### <a id="runHA"></a>Restore an HA Cluster Instance

<p class="note warning"><strong>WARNING</strong>: This is a destructive action and
  should only be run on a new and unused service instance.</p>

Be sure you have followed the procedure in
[Create and Prepare a New Service Instance for Restore](#prepare) above.

To restore an HA cluster, do the following:

1. To pause the local database server, run the following command:

    ```
    monit stop all
    ```
1. To confirm that all jobs are listed as `not monitored`, run the following command:

    ```
    watch monit summary
    ```
    <!-- What happends if it is `monitored` -->
1. To delete the existing MySQL data that is stored on disk, run the following command:

    ```
    rm -rf /var/vcap/store/mysql/*
    ```

1. Move the compressed backup file to the node using `scp`.
1. To decrypt and expand the file using `gpg`, sending the output to tar:

    ```
    gpg --decrypt mysql-backup.tar.gpg | tar -C /var/vcap/store/mysql -xvf -
    ```
1. To change the owner of the data directory, run the following:

     ```
     chown -R vcap:vcap /var/vcap/store/mysql
     ```

     MySQL expects the data directory to be owned by a particular user.
1. To start all services with `monit`, run the following command:

    ```
    monit start all
    ```

1. To watch the summary until all jobs are listed as `running`, run the following command:

    ```
    watch monit summary
    ```
1. Exit out of the MySQL node.

#### <a id="restage"></a> Re-stage the Service Instance

After you restore your single node, leader-follower, or HA cluster service instance,
you must re-stage your new service instance.

To re-stage your service instance, do the following:

1. If you restored to a service instance with a single-node plan but want a
leader-follower plan, update the plan now:

    ```
    cf update-service NEW-INSTANCE-NAME -p LEADER-FOLLOWER-PLAN
    ```

1. If you scaled instances to one in step 6 of *Create and Prepare a New Service
Instance for Restore* above, do the following:
    1. To scale your instance, do one of the following:
      + If you scaled a leader-follower instance, modify the deployment manifest
      to edit the `instances` property to `2` and the `leader-follower` property to `true`.
      Update the instance group and properties for your new service instance as follows:

            ```yaml
            instance_groups:
            - name: mysql
              ...
              instances: 2    # Scale instances to 2
              ...
            properties:
              leader_follower:
                enabled: true    # Set to true
              ...

             ```
      + If you scaled an HA cluster instance, set the `instances` property to `3` by
      modifying the deployment manifest.
      Update the instance group for your new service instance as follows:

            ```yaml
            instance_groups:
            - name: mysql
              ...
              instances: 3    # Scale instances to 3
              ...
             ```
  1. Redeploy the deployment manifest by running:

        ```
        bosh -e ENVIRONMENT -d DEPLOYMENT deploy PATH-TO-MANIFEST.yml
        ```

        For example:

        <pre class="terminal">
        bosh -e my-env -d service-instance\_12345678-90ab-cdef-1234-567890abcdef deploy ./manifest.yml
        </pre>

1. Determine if the app is currently bound to a MySQL service instance:

    ```
    cf services
    ```

1. If the previous step shows that the app is currently bound to a MySQL instance, unbind it:

    ```
    cf unbind-service MY-APP OLD-INSTANCE-NAME
    ```

1. Update your CF app to bind to the new service instance:

    ```
    cf bind-service MY-APP  NEW-INSTANCE-NAME
    ```

1. Restage your CF app to make the changes take effect:

    ```
    cf restage MY-APP
    ```

Your app should be running and able to access the restored data.
