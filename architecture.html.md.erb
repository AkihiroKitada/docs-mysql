---
title: Architecture
owner: MySQL
---

This topic describes the high-level component architecture of the MySQL for Pivotal Cloud Foundry (PCF) service, in both its single-node and HA configurations. It also describes the defaults and other design decisions applied to the service's MariaDB servers and the Galera Cluster that manages them.

## <a id="topology"></a> HA Topology ##

In HA topology, the MySQL for PCF service uses the following:

* Three MySQL servers running [MariaDB](https://mariadb.com/kb/en/mariadb/what-is-mariadb-galera-cluster/) as the MySQL engine and [Galera](http://galeracluster.com) to keep them in sync

* Two [Switchboard](https://github.com/cloudfoundry-incubator/switchboard) proxies that direct queries to a single backend, and provide superfast failover in the event of a backend server failure

* A customer-provided load balancer that directs traffic to one of the proxies, or to the other proxy in the event of proxy failure

* Two service brokers that create and bind service instances with failover in the event of a service broker failure

If you use multiple availability zones, the MySQL installer spreads MySQL and Proxy components across zones to make the service durable in the event of a zone failure.

![Quorum](topology.png)

### <a id="how-it-works"></a> How It Works

Normally, MySQL for PCF only sends queries to one of the three MySQL servers at a time, the _current master_ node. Galera ensures that the other two servers mirror any changes made on the current master.

In a three-node Galera cluster, two nodes define a quorum that automatically maintains availability when any node becomes inaccessible. This happens when the host fails or in the event of a network partition.

When a single node cannot communicate with the rest of the cluster, it stops accepting traffic. The other two nodes continue to function normally. When the isolated node regains connectivity, it rejoins the cluster.

If two nodes are no longer able to connect to the cluster, quorum is lost and  the cluster is no longer accessible. To bring the cluster back up, you must restart it manually, as documented in the [bootstrapping](bootstrapping.html) topic.

### <a id="even-number"></a> Avoid an Even Number of Nodes ###

Pivotal recommends deploying either one or three MySQL server nodes, avoiding an even number. This prevents a network partition from causing the entire cluster to lose quorum, because neither side has more than half of the nodes.

The minimum number of nodes required to tolerate a single node failure is three. With a two-node cluster, a single node failure causes loss of quorum.

## <a id="defaults"></a> MySQL Server Defaults 

This section describes the defaults that the MySQL for PCF tile applies to its Galera and MariaDB components.

### SST method: xtrabackup

Galera supports multiple methods for [State Snapshot Transfer](http://www.percona.com/doc/percona-xtradb-cluster/5.5/manual/state_snapshot_transfer.html).
The `rsync` method is usually fastest. The `xtrabackup` method has the advantage of keeping the donor node writeable during SST. We have chosen to use `xtrabackup`.

### InnoDB Log Files: 1GB

Our cluster defaults to 1GB for log file size to support larger blob.

### Max User Connections: 40

To ensure all users get fair access to system resources, we default each user's number of connections to 40. Operators can override this setting, per plan, in the **Service Plans** configuration pane.

### Skip External Locking

Each virtual machine only runs one `mysqld` process, so they do not need external locking.

### Max Allowed Packet: 256MB

We allow blobs up to 256MB. This size is unlikely to limit a user's query, but is also manageable for our InnoDB log file size. Consider using our Riak CS service for storing large files.

### InnoDB File-Per-Table Tablespaces

InnoDB allows using either a single file to represent all data, or a separate file for each table. We chose to use a separate file for each table as this provides more flexibility and optimization. For a full list of pros and cons, see MySQL's documentation for [InnoDB File-Per-Table Mode](http://dev.mysql.com/doc/refman/5.5/en/innodb-multiple-tablespaces.html).

### InnoDB File Format: Barracuda

To take advantage of all the extra features available with the `innodb_file_per_table = ON` option, we use the `Barracuda` file format.

### Temporary Tables

MySQL converts temporary in-memory tables to temporary on-disk tables when either of the following occurs:

* A query generates more than 16 million rows of output
* A query uses more than 32MB of data space

Users can see if a query is using a temporary table by using the `EXPLAIN` command and looking for `Using temporary` in the output.

If a server simultaneously processes multiple very large queries that use `/tmp` space, queries may return `no space left` errors.

### Reverse Name Resolution Off

MySQL for PCF authenticates with user credentials, which does not restrict the host from which a user connects. To save time on each new connection, the service disables reverse name resolution by default.

### Large Data File Splitting Enabled

MySQL for PCF enables `wsrep_load_data_splitting` to split large data imports into separate transactions. This facilitates loading large files into a MariaDB cluster.

## <a id="behavior"></a> Cluster Behavior ##

This section explains how the MySQL cluster behaves when you change its number of nodes.

### Scaling Up from One to Three Nodes

When you add new MariaDB nodes to an existing node, they replicate data from the existing primary node and join the cluster once replication is complete. Performance may temporarily degrade while the new nodes sync all data from the original node.

After new nodes have joined the cluster, the proxy continues to route incoming connections to the primary node.

If the proxy detects that the primary node is [unhealthy](https://github.com/cloudfoundry/cf-mysql-release/blob/release-candidate/docs/proxy.md#unhealthy), it severs connections to the primary node and routes all new connections to a different, healthy node.

If the proxy detects that no healthy MariaDB nodes exist in the cluster, it rejects all connections.

### Scaling Down from Three to One Node

When you scale down from multiple MariaDB nodes to a single node, the primary node continues survives as the sole remaining node (provided it remains healthy), and the proxy continues to route incoming connections to the node.

### <a id="removal"></a> Graceful Removal of Nodes

Removing nodes from a cluster _gracefully_ means that the cluster size decreases while the cluster maintains its healthy state.

You can remove nodes gracefully by manually shutting each down with `monit` or by decreasing the cluster size as described in the [Switch Between Single and HA Topologies](./configuring.html#switch-topologies) section of the Configuring MySQL for PCF topic.

### <a id="rejoin"></a> Rejoining the Cluster (Existing Nodes)

Existing nodes restarted with `monit` should automatically join the cluster. If an existing node fails to join the cluster, it may be because its transaction record's (`seqno`) is higher than that of the nodes in the cluster with quorum (aka the primary component).

  - If the node has a higher `seqno` it will be apparent in the error log `/var/vcap/sys/log/mysql/mysql.err.log`.
  - If the healthy nodes of a cluster have a lower transaction record number than the failing node, it might be desirable to shut down the healthy nodes and bootstrap from the node with the more recent transaction record number. See the [bootstrapping](bootstrapping.html) topic for more details.
  - Manual recovery may be possible, but is error-prone and involves dumping transactions and applying them to the running cluster (out of scope for this doc).
  - Abandoning the data is also an option, if you're OK with losing the unsynchronized transactions. Follow the following steps to abandon the data (as root):
    - Stop the process with `monit stop mariadb_ctrl`.
    - Delete the Galera state (`/var/vcap/store/mysql/grastate.dat`) and cache (`/var/vcap/store/mysql/galera.cache`) files from the persistent disk.
    - Restarting the node with `monit start mariadb_ctrl`.

### <a id='state-snapshot-transfer-sst'></a>State Snapshot Transfer (SST)

When a new node is added to the cluster or rejoins the cluster, it synchronizes state with the primary component via a process called SST. A single node from the primary component is chosen to act as a state donor. By default Galera uses `rsync` to perform SST, which blocks for the duration of the transfer. However, MySQL for Pivotal Cloud Foundry (PCF) uses [Xtrabackup](http://www.percona.com/doc/percona-xtrabackup), which allows the donor node to continue to accept reads and writes.

