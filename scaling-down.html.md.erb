---
title: Scaling Down MySQL
owner: MySQL
---

This topic describes how to safely scale down your MySQL for Pivotal Cloud Foundry (PCF) cluster to a single node.

By default MySQL for PCF is a single node. To take advantage of the high availability features of MySQL for PCF, you may have scaled the configuration up to three nodes.

<p class="note"><strong>Note</strong>: If you are only running the MySQL cluster with a single node, you do not need to perform these steps.</p>

## <a id="check-health"></a>Check the Health of Your Cluster

Before scaling down your MySQL cluster, perform the following actions to ensure the cluster is healthy.

1. Obtain the IP addresses of your MySQL server by performing the following steps:
	1. From the Pivotal Cloud Foundry (PCF) **Installation Dashboard**, click the **MySQL for Pivotal Cloud Foundry** tile.
	1. Click the **Status** tab.
	1. Record the IP addresses for all instances of the **MySQL Server** job.

1. Obtain the admin credentials for your MySQL server by performing the following steps:
	1. From the MySQL for PCF tile, click the **Credentials** tab.
	1. Locate the **Mysql Admin Password** entry in the **MySQL Server** section and click **Link to Credential**.
	1. Record the values for `identity` and `password`.

1. SSH into the Ops Manager VM. Because the procedures vary by IaaS, review the [SSH into Ops Manager](http://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#ssh) section of the Advanced Troubleshooting with the BOSH CLI topic for specific instructions.

1. From the Ops Manager VM, place some data in the first node by performing the following steps, replacing `FIRST-NODE-IP-ADDRESS` with the IP address of the first node retrieved above and `YOUR-IDENTITY` with the `identity` value obtained above. When prompted for a password, provide the `password` value obtained above.
	1. Create a dummy database in the first node:
		<pre class="terminal">
		$ mysql -h FIRST-NODE-IP-ADDRESS -u YOUR-IDENTITY -p -e "create database verify\_healthy;"
		</pre>
	1. Create a dummy table in the dummy database:
		<pre class="terminal">
		$ mysql -h FIRST-NODE-IP-ADDRESS -u YOUR-IDENTITY -p -D verify\_healthy -e "create table dummy_table (id int not null primary key auto\_increment, info text) engine='InnoDB';"
		</pre>
	1. Insert some data into the dummy table:
		<pre class="terminal">
		$ mysql -h FIRST-NODE-IP-ADDRESS -u YOUR-IDENTITY -p -D verify\_healthy -e "insert into dummy_table(info) values ('dummy data'),('more dummy data'),('even more dummy data');"
		</pre>
	1. Query the table and verify that the three rows of dummy data exist on the first node:
		<pre class="terminal">
		mysql -h FIRST-NODE-IP-ADDRESS -u YOUR-IDENTITY -p -D verify\_healthy -e "select * from dummy\_table;"
		Enter password:
		+----+----------------------+
		| id | info                 |
		+----+----------------------+
		|  4 | dummy data           |
		|  7 | more dummy data      |
		| 10 | even more dummy data |
		+----+----------------------+
	</pre>

1. Verify that the other nodes contain the same dummy data by performing the following steps for each of the remaining MySQL server IP addresses obtained above:
	1. Query the dummy table, replacing `NEXT-NODE-IP-ADDRESS` with the IP address of the MySQL server instance and `YOUR-IDENTITY` with the `identity` value obtained above. When prompted for a password, provide the `password` value obtained above.
		<pre class="terminal">
	 	$ mysql -h NEXT-NODE-IP-ADDRESS -u YOUR-IDENTITY -p -D verify\_healthy -e "select * from dummy\_table;"
	 	</pre>
	1. Examine the output of the `mysql` command and verify that the node contains the same three rows of dummy data as the other nodes.
 		<pre class="terminal">
		+----+----------------------+
		| id | info                 |
		+----+----------------------+
		|  4 | dummy data           |
		|  7 | more dummy data      |
		| 10 | even more dummy data |
		+----+----------------------+
		</pre>

1. If each MySQL server instance does not return the same result, contact [Pivotal Support](https://support.pivotal.io/) before proceeding further or making any changes to your deployment. If each MySQL server instance does return the same result, then you can safely proceed to scaling down your cluster to a single node by performing the steps in the following section.

## <a id="scale-down"></a>Scale Down Your Cluster

1. Delete the dummy database, replacing `FIRST-NODE-IP-ADDRESS` with the IP address of the first MySQL server node and `YOUR-IDENTITY` with the `identity` value obtained above. When prompted for a password, provide the `password` value obtained above.
	<pre class="terminal">
	$ mysql -h FIRST-NODE-IP-ADDRESS -u YOUR-IDENTITY -p -e "drop database verify\_healthy;"
	</pre>
1. From the PCF **Installation Dashboard**, click the **MySQL for Pivotal Cloud Foundry** tile.
1. Click the **Settings** tab.
1. Click **Resource Config** and use the drop-down menu to change the **Instances** count for **MySQL Server** to `1`.
1. Click **Save** to apply the changes.

## <a id="add-nodes"></a> Adding new nodes ##

When new nodes are added to or removed from a MySQL service, a top-level property is updated with the new nodes' IP addresses. As BOSH deploys, it will update the configuration and restart all of the MySQL nodes **and** the proxy nodes (to inform them of the new IP addresses as well). Restarting the nodes will cause all connections to that node to be dropped while the node restarts.

## <a id="scaling"></a> Scaling the cluster ##

#### Scaling up from 1 to N nodes

When a new MariaDb node comes online, it replicates data from the existing node in the cluster. Once replication is complete, the node will join the cluster. The proxy will continue to route all incoming connections to the primary node while it remains healthy.

If the proxy detects that this node becomes [unhealthy](https://github.com/cloudfoundry/cf-mysql-release/blob/release-candidate/docs/proxy.md#unhealthy), it will sever existing connections, and route all new connections to a different, healthy node. If there are no healthy MariaDb nodes, the proxy will reject all subsequent connections.

While transitioning from one node to a cluster, there will be an undetermined period of performance degradation while the new node syncs all data from the original node.

Note: If you are planning to scale up MariaDb nodes, it is recommended to do so in different Availability Zones to maximize cluster availability. An Availability Zone is a network-distinct section of a given Region. Further details are available in [Amazon's documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html).

#### Scaling down from N to 1 node
When scaling from multiple nodes to a single MariaDb node, the proxy will determine that the sole remaining node is the primary node (provided it remains healthy). The proxy routes incoming connections to the remaining MariaDb node.

### <a id="removal"></a> Graceful removal of a node ###
  - Shutting down a node with monit (or decreasing cluster size by one) will cause the node to gracefully leave the cluster.
  - Cluster size is reduced by one and maintains healthy state. Cluster will continue to operate, even with a single node, as long as other nodes left gracefully.

### <a id="rejoin"></a> Rejoining the cluster (existing nodes) ##
Existing nodes restarted with monit should automatically join the cluster. If an existing node fails to join the cluster, it may be because its transaction record's (`seqno`) is higher than that of the nodes in the cluster with quorum (aka the primary component).

  - If the node has a higher `seqno` it will be apparent in the error log `/var/vcap/sys/log/mysql/mysql.err.log`.
  - If the healthy nodes of a cluster have a lower transaction record number than the failing node, it might be desirable to shut down the healthy nodes and bootstrap from the node with the more recent transaction record number. See the [bootstrapping docs](bootstrapping.html) for more details.
  - Manual recovery may be possible, but is error-prone and involves dumping transactions and applying them to the running cluster (out of scope for this doc).
  - Abandoning the data is also an option, if you're ok with losing the unsynchronized transactions. Follow the following steps to abandon the data (as root):
    - Stop the process with `monit stop mariadb_ctrl`.
    - Delete the galera state (`/var/vcap/store/mysql/grastate.dat`) and cache (`/var/vcap/store/mysql/galera.cache`) files from the persistent disk.
    - Restarting the node with `monit start mariadb_ctrl`.

### <a id='state-snapshot-transfer-sst'></a>State Snapshot Transfer (SST)

When a new node is added to the cluster or rejoins the cluster, it synchronizes state with the primary component via a process called SST. A single node from the primary component is chosen to act as a state donor. By default Galera uses rsync to perform SST, which blocks for the duration of the transfer. However, MySQL for Pivotal Cloud Foundry (PCF) is configured to use [Xtrabackup](http://www.percona.com/doc/percona-xtrabackup), which allows the donor node to continue to accept reads and writes.
