---
title: Monitoring the MySQL Service
owner: MySQL
---

This document describes how to use the Replication Canary and Interruptor to monitor your MySQL cluster. 

## <a id="repcanary"></a>Replication Canary

The Replication Canary is a long-running monitor that validates that replication is working within the MySQL cluster. The clustered solution used by the MySQL for Pivotal Cloud Foundry service provides many benefits, such as quick failover and rolling upgrades. This is more complex than a single node system with no replication. The Replication Canary verifies that the clustering technology is working according to specification.

### <a id="how"></a>How it Works

The Replication Canary writes to a private dataset on the cluster, then ensures that it is able to read that data from each of the nodes of the cluster. It pauses for a period between write and the reads to ensure that the writesets have been committed across each node of the cluster. The size of the private dataset is constrained, and does not use a significant amount of disk capacity.

When replication is not working properly, the Canary will detect that it is not able to read the data from all nodes, and will immediately escalate. **Malfunctioning replication exposes the cluster to the possibility of data loss.** The Canary takes two actions:

  - E-mails a pre-configured address with a message that replication has failed (see sample below).
  - Disables client access to the cluster.

  <p class='note'><strong>Note</strong>: Due to the serious nature of a failure in replication, both behaviors are enabled by default. It is critical that you contact Pivotal support immediately.</strong> Support will work with you to determine the nature of the cluster's failure, and advise a suggested resolution.</p>

Configure the Replication Canary to e-mail an e-mail address that is closely monitored. The purpose of the Canary is to escalate replication failure as quickly as possible.

### <a id="disable-access"></a>Disabling Cluster Access

For each attempt that detects that cluster replication is not working properly, the Canary will instruct all Proxies to disable connections to the database cluster.

The Canary continues to run. Should the replication issue be resolved, the Canary will detect this and automatically restore client access to the cluster.

If you must restore access to the cluster regardless of the Replication Canary, please contact Support.

#### Determining Proxy State

The Proxy offers an API to determine if access is currently disabled. This is an example:

  <pre class="terminal">
  ubuntu@ip-10-0-0-38:~$ curl -ku admin:PASSWORD\_FROM\_OPSMGR -X GET https://proxy-0-p-mysql.SYSTEM-DOMAIN/v0/cluster ; echo
  {"currentBackendIndex":0,"trafficEnabled":false,"message":"Disabling cluster traffic","lastUpdated":"2016-07-27T05:16:29.197754077Z"}
  </pre>


### <a id="enable-canary"></a>Enable the Replication Canary

#### Configure the Elastic Runtime Tile

<p class="note"><strong>Note</strong>: In a typical PCF deployment, these settings are already configured.
</p>
1. In the **SMTP Config** section, enter a **From Email** that the Replication Canary can use to send notifications, along with the SMTP server configuration.
1. In the **Errands** section, select the **Notifications** errand.


#### Configure the MySQL for Pivotal Cloud Foundry (PCF) Tile

1. In the **Advanced Options** section, select **Enable replication canary**.
    ![Enable Replication](enable-replication.png)
1. If you want to the Replication Canary to send e-mail only, and not disable access at the proxy, select **Notify only**. 
  <p class="note"><strong>Note</strong>: Pivotal recommends leaving this checkbox unselected due to the serious nature of replication failure.</p>
1. You can override the **Replication canary time period**. The **Replication canary time period** addresses how frequently the canary checks for replication failure, in seconds. This adds a small amount of load to the databases, but the canary reacts more quickly to replication failure. The default is 30 seconds.

    ![Canary time](canary_time.png)

1. You can override the **Replication canary read delay**. The **Replication canary read delay** addresses how long the canary waits to verify data is replicating across each MySQL node, in seconds. Clusters under heavy load experience some small replication lag as writesets are committed across the nodes (see above). Default is 20 seconds.
1. Enter an **E-mail address** to receive monitoring notifications.
1. In the **Resource Config** section, ensure the **Monitoring** job has one instance.
  ![Resource config](resource-config.png)

### Disable the Replication Canary

If you do not need the Replication Canary, for instance if you use a single MySQL node, follow this procedure to disable both the job and the resource configuration.

1. In the MySQL for Pivotal Cloud Foundry tile:

In the **Advanced Options** section of the MySQL for PCF tile, e “Disable Replication Canary” radio button is selected.

![Disable protection](disable-protection.png)

**Important:** On the **Resource Config** pane, ensure the “Monitoring” job is configured to zero instances.

![Monitoring zero](monitoring-zero.png)

### Sample notification e-mail

Immediately upon recognizing that replication is not working across the cluster, the Canary will begin to send e-mail via Elastic Runtime's Notification service. This is a sample of the wording to expect.

    Subject: CF Notification: p-mysql Replication Canary, alert 417

    This message was sent directly to your email address.

    {alert-code 417}
    This is an e-mail to notify you that the MySQL service's replication canary has detected an unsafe cluster condition in which replication is not performing as expected across all nodes.

## <a id="interruptor"></a>Interruptor

There are rare instances when a MySQL node silently falls out of sync with the other nodes in the cluster. This condition is closely monitored by the Replication Canary (above). However, in case the Replication Canary misses a failure condition, the Interruptor is a solution to prevent data loss.

### How it Works

In the instance where the node receiving traffic from the proxy falls out of sync with the other nodes in the cluster, it will generate a dataset which is not shared by the other nodes. If at some point, the node receives a transaction which is not compatible with the datasets of the other nodes, the standard behavior of the node receiving the transaction is to discard its local dataset and adopt the datasets of the other nodes in the cluster. While this is normally desiarable behavior, in the case where data replication is not functioning across the cluster, it is possible that in discarding the local dataset, the node is destroying valid data, resulting in data loss.

When enabled, the Interruptor will prevent a node from destroying its local dataset only under the circumstance that there is a risk of destroying valid data.

  <p class='note'><strong>Note</strong>: Should you receive notification that the Interruptor has activated, it is critical that you contact Pivotal support immediately.</strong> Support will work with you to determine the nature of the node's failure, and advise a suggested resolution.</p>

There are times when a node legitimately falls behind, and it is safe to discard the node's dataset. There are [two modes](http://galeracluster.com/documentation-webpages/statetransfer.html) employed by a node to catch up when it has been out of sync with the cluster for some time. When a node has been out of the cluster for a relatively short period of time (for example, over the course of a reboot), the node invokes _Incremental State Transfer_. This is not a dangerous operation, and the Interruptor does not interfere with IST activity. However, when a node has been unavailable for an extended amount of time (for example, a hardware failure that requires physical repair) then the node may invoke _State Snapshot Transfer_. In cases where cluster replication has failed, SST is possibly dangerous. When active, the Interruptor prevents this method of recovery.

### Interruptor Logs

You can confirm that the Interruptor has activated by examining the `/var/vcap/sys/log/mysql/mysql.err.log` on the failing node. The log will contain the following:

  <pre class="terminal">
  WSREP\_SST: [ERROR] ############################################################################## (20160610 04:33:21.338)
  WSREP\_SST: [ERROR] SST disabled due to danger of data loss. Verify data and bootstrap the cluster (20160610 04:33:21.340)
  WSREP\_SST: [ERROR] ############################################################################## (20160610 04:33:21.341)
  </pre>

### Force a Node to Rejoin the Cluster

In most cases where the Interruptor has activated, but the Replication Canary has not triggered, it's safe for the node to rejoin the cluster.

<p class='note'><strong>Note</strong>: This topic requires you to run commands from the <a href="http://docs.pivotal.io/pivotalcf/customizing/index.html">Ops Manager Director</a> using the BOSH CLI. Refer to the <a href="http://docs.pivotal.io/pivotalcf/customizing/trouble-advanced.html#prepare">Advanced Troubleshooting with the BOSH CLI</a> topic for more information.</p>

  - Choose the p-mysql manifest, following these [instructions](bootstrapping.html#manifest).
  - Force a node to rejoin the cluster: `bosh run errand rejoin-unsafe`

    Sample output:
    <pre class="terminal">
    $ bosh run errand rejoin-unsafe
    [...]
    [stdout]
    Started rejoin-unsafe errand ...
    Successfully repaired cluster
    rejoin-unsafe errand completed

    [stderr]
    None

    Errand `rejoin-unsafe' completed successfully (exit code 0)
    </pre>

### Sample notification e-mail

Immediately upon preventing a node from rejoining a cluster, the Interruptor will send e-mail via Elastic Runtime's Notification service. This is a sample of the wording to expect.

    Subject: CF Notification: p-mysql alert 100

    This message was sent directly to your email address.

    {alert-code 100}
    Hello, just wanted to let you know that the MySQL node/cluster has gone down and has been disallowed from re-joining by the interruptor.

### Disable the Interruptor

The Interruptor is enabled by default. To disable the Interruptor:

In the **Advanced Options** section, under **Enable optional protections**, un-check  **Prevent node auto re-join**.

  ![Prevent node](prevent-node.png)
